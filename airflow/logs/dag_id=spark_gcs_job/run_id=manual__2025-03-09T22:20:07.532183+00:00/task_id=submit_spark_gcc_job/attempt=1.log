[2025-03-09T22:20:10.412+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-09T22:20:10.453+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_gcs_job.submit_spark_gcc_job manual__2025-03-09T22:20:07.532183+00:00 [queued]>
[2025-03-09T22:20:10.474+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_gcs_job.submit_spark_gcc_job manual__2025-03-09T22:20:07.532183+00:00 [queued]>
[2025-03-09T22:20:10.474+0000] {taskinstance.py:2866} INFO - Starting attempt 1 of 2
[2025-03-09T22:20:10.514+0000] {taskinstance.py:2889} INFO - Executing <Task(SparkSubmitOperator): submit_spark_gcc_job> on 2025-03-09 22:20:07.532183+00:00
[2025-03-09T22:20:10.534+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'spark_gcs_job', 'submit_spark_gcc_job', 'manual__2025-03-09T22:20:07.532183+00:00', '--job-id', '390', '--raw', '--subdir', 'DAGS_FOLDER/spark_submit.py', '--cfg-path', '/tmp/tmp8si08wn6']
[2025-03-09T22:20:10.545+0000] {standard_task_runner.py:105} INFO - Job 390: Subtask submit_spark_gcc_job
[2025-03-09T22:20:10.552+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=567) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-03-09T22:20:10.553+0000] {standard_task_runner.py:72} INFO - Started process 568 to run task
[2025-03-09T22:20:10.686+0000] {task_command.py:467} INFO - Running <TaskInstance: spark_gcs_job.submit_spark_gcc_job manual__2025-03-09T22:20:07.532183+00:00 [running]> on host 27f453beda85
[2025-03-09T22:20:10.990+0000] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='spark_gcs_job' AIRFLOW_CTX_TASK_ID='submit_spark_gcc_job' AIRFLOW_CTX_EXECUTION_DATE='2025-03-09T22:20:07.532183+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-03-09T22:20:07.532183+00:00'
[2025-03-09T22:20:10.991+0000] {taskinstance.py:731} INFO - ::endgroup::
[2025-03-09T22:20:11.122+0000] {base.py:84} INFO - Retrieving connection 'spark_default'
[2025-03-09T22:20:11.124+0000] {spark_submit.py:474} INFO - Spark-Submit cmd: spark-submit --master spark://spark:7077 --conf spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem --conf spark.hadoop.fs.gs.auth.service.account.enable=true --conf spark.hadoop.fs.gs.auth.service.account.json.keyfile=/opt/***/keys/first-key.json --jars /opt/***/jars/gcs-connector-hadoop3-2.2.0.jar --total-executor-cores 2 --executor-memory 2g --name spark_gcs_job --verbose --deploy-mode client /opt/***/jobs/gcc_test_job.py
[2025-03-09T22:20:11.551+0000] {spark_submit.py:641} INFO - /home/***/.local/lib/python3.12/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2025-03-09T22:20:16.014+0000] {spark_submit.py:641} INFO - Using properties file: null
[2025-03-09T22:20:16.202+0000] {spark_submit.py:641} INFO - Parsed arguments:
[2025-03-09T22:20:16.203+0000] {spark_submit.py:641} INFO - master                  spark://spark:7077
[2025-03-09T22:20:16.203+0000] {spark_submit.py:641} INFO - remote                  null
[2025-03-09T22:20:16.203+0000] {spark_submit.py:641} INFO - deployMode              client
[2025-03-09T22:20:16.204+0000] {spark_submit.py:641} INFO - executorMemory          2g
[2025-03-09T22:20:16.204+0000] {spark_submit.py:641} INFO - executorCores           null
[2025-03-09T22:20:16.204+0000] {spark_submit.py:641} INFO - totalExecutorCores      2
[2025-03-09T22:20:16.204+0000] {spark_submit.py:641} INFO - propertiesFile          null
[2025-03-09T22:20:16.204+0000] {spark_submit.py:641} INFO - driverMemory            null
[2025-03-09T22:20:16.204+0000] {spark_submit.py:641} INFO - driverCores             null
[2025-03-09T22:20:16.204+0000] {spark_submit.py:641} INFO - driverExtraClassPath    null
[2025-03-09T22:20:16.205+0000] {spark_submit.py:641} INFO - driverExtraLibraryPath  null
[2025-03-09T22:20:16.205+0000] {spark_submit.py:641} INFO - driverExtraJavaOptions  null
[2025-03-09T22:20:16.205+0000] {spark_submit.py:641} INFO - supervise               false
[2025-03-09T22:20:16.205+0000] {spark_submit.py:641} INFO - queue                   null
[2025-03-09T22:20:16.205+0000] {spark_submit.py:641} INFO - numExecutors            null
[2025-03-09T22:20:16.205+0000] {spark_submit.py:641} INFO - files                   null
[2025-03-09T22:20:16.206+0000] {spark_submit.py:641} INFO - pyFiles                 null
[2025-03-09T22:20:16.206+0000] {spark_submit.py:641} INFO - archives                null
[2025-03-09T22:20:16.206+0000] {spark_submit.py:641} INFO - mainClass               null
[2025-03-09T22:20:16.206+0000] {spark_submit.py:641} INFO - primaryResource         file:/opt/***/jobs/gcc_test_job.py
[2025-03-09T22:20:16.206+0000] {spark_submit.py:641} INFO - name                    spark_gcs_job
[2025-03-09T22:20:16.206+0000] {spark_submit.py:641} INFO - childArgs               []
[2025-03-09T22:20:16.207+0000] {spark_submit.py:641} INFO - jars                    file:/opt/***/jars/gcs-connector-hadoop3-2.2.0.jar
[2025-03-09T22:20:16.207+0000] {spark_submit.py:641} INFO - packages                null
[2025-03-09T22:20:16.207+0000] {spark_submit.py:641} INFO - packagesExclusions      null
[2025-03-09T22:20:16.208+0000] {spark_submit.py:641} INFO - repositories            null
[2025-03-09T22:20:16.208+0000] {spark_submit.py:641} INFO - verbose                 true
[2025-03-09T22:20:16.208+0000] {spark_submit.py:641} INFO - 
[2025-03-09T22:20:16.209+0000] {spark_submit.py:641} INFO - Spark properties used, including those specified through
[2025-03-09T22:20:16.209+0000] {spark_submit.py:641} INFO - --conf and those from the properties file null:
[2025-03-09T22:20:16.209+0000] {spark_submit.py:641} INFO - (spark.hadoop.fs.gs.auth.service.account.enable,true)
[2025-03-09T22:20:16.209+0000] {spark_submit.py:641} INFO - (spark.hadoop.fs.gs.auth.service.account.json.keyfile,/opt/***/keys/first-key.json)
[2025-03-09T22:20:16.210+0000] {spark_submit.py:641} INFO - (spark.hadoop.fs.gs.impl,com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem)
[2025-03-09T22:20:16.210+0000] {spark_submit.py:641} INFO - 
[2025-03-09T22:20:16.210+0000] {spark_submit.py:641} INFO - 
[2025-03-09T22:20:16.934+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-03-09T22:20:17.388+0000] {spark_submit.py:641} INFO - Main class:
[2025-03-09T22:20:17.388+0000] {spark_submit.py:641} INFO - org.apache.spark.deploy.PythonRunner
[2025-03-09T22:20:17.389+0000] {spark_submit.py:641} INFO - Arguments:
[2025-03-09T22:20:17.389+0000] {spark_submit.py:641} INFO - file:/opt/***/jobs/gcc_test_job.py
[2025-03-09T22:20:17.389+0000] {spark_submit.py:641} INFO - null
[2025-03-09T22:20:17.391+0000] {spark_submit.py:641} INFO - Spark config:
[2025-03-09T22:20:17.391+0000] {spark_submit.py:641} INFO - (spark.app.name,spark_gcs_job)
[2025-03-09T22:20:17.392+0000] {spark_submit.py:641} INFO - (spark.app.submitTime,1741558817365)
[2025-03-09T22:20:17.392+0000] {spark_submit.py:641} INFO - (spark.cores.max,2)
[2025-03-09T22:20:17.392+0000] {spark_submit.py:641} INFO - (spark.executor.memory,2g)
[2025-03-09T22:20:17.392+0000] {spark_submit.py:641} INFO - (spark.hadoop.fs.gs.auth.service.account.enable,true)
[2025-03-09T22:20:17.393+0000] {spark_submit.py:641} INFO - (spark.hadoop.fs.gs.auth.service.account.json.keyfile,/opt/***/keys/first-key.json)
[2025-03-09T22:20:17.393+0000] {spark_submit.py:641} INFO - (spark.hadoop.fs.gs.impl,com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem)
[2025-03-09T22:20:17.393+0000] {spark_submit.py:641} INFO - (spark.jars,file:///opt/***/jars/gcs-connector-hadoop3-2.2.0.jar)
[2025-03-09T22:20:17.394+0000] {spark_submit.py:641} INFO - (spark.master,spark://spark:7077)
[2025-03-09T22:20:17.394+0000] {spark_submit.py:641} INFO - (spark.repl.local.jars,file:///opt/***/jars/gcs-connector-hadoop3-2.2.0.jar)
[2025-03-09T22:20:17.394+0000] {spark_submit.py:641} INFO - (spark.submit.deployMode,client)
[2025-03-09T22:20:17.395+0000] {spark_submit.py:641} INFO - (spark.submit.pyFiles,)
[2025-03-09T22:20:17.395+0000] {spark_submit.py:641} INFO - Classpath elements:
[2025-03-09T22:20:17.395+0000] {spark_submit.py:641} INFO - file:///opt/***/jars/gcs-connector-hadoop3-2.2.0.jar
[2025-03-09T22:20:17.396+0000] {spark_submit.py:641} INFO - 
[2025-03-09T22:20:17.396+0000] {spark_submit.py:641} INFO - 
[2025-03-09T22:20:19.531+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:19 INFO SparkContext: Running Spark version 3.5.5
[2025-03-09T22:20:19.531+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:19 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-03-09T22:20:19.531+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:19 INFO SparkContext: Java version 17.0.14
[2025-03-09T22:20:19.570+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:19 INFO ResourceUtils: ==============================================================
[2025-03-09T22:20:19.571+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:19 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-03-09T22:20:19.571+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:19 INFO ResourceUtils: ==============================================================
[2025-03-09T22:20:19.571+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:19 INFO SparkContext: Submitted application: GCSConnectorTest
[2025-03-09T22:20:19.603+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-03-09T22:20:19.613+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:19 INFO ResourceProfile: Limiting resource is cpu
[2025-03-09T22:20:19.613+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:19 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-03-09T22:20:19.757+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:19 INFO SecurityManager: Changing view acls to: ***
[2025-03-09T22:20:19.758+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:19 INFO SecurityManager: Changing modify acls to: ***
[2025-03-09T22:20:19.759+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:19 INFO SecurityManager: Changing view acls groups to:
[2025-03-09T22:20:19.760+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:19 INFO SecurityManager: Changing modify acls groups to:
[2025-03-09T22:20:19.760+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2025-03-09T22:20:20.288+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:20 INFO Utils: Successfully started service 'sparkDriver' on port 33047.
[2025-03-09T22:20:20.351+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:20 INFO SparkEnv: Registering MapOutputTracker
[2025-03-09T22:20:20.417+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:20 INFO SparkEnv: Registering BlockManagerMaster
[2025-03-09T22:20:20.461+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-03-09T22:20:20.462+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-03-09T22:20:20.468+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-03-09T22:20:20.513+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-37978b35-2cee-4bd6-97bb-b3a732193c77
[2025-03-09T22:20:20.532+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:20 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-03-09T22:20:20.553+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:20 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-03-09T22:20:20.783+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:20 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-03-09T22:20:20.900+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-03-09T22:20:20.950+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:20 INFO SparkContext: Added JAR file:///opt/***/jars/gcs-connector-hadoop3-2.2.0.jar at spark://27f453beda85:33047/jars/gcs-connector-hadoop3-2.2.0.jar with timestamp 1741558819511
[2025-03-09T22:20:21.064+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:21 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark:7077...
[2025-03-09T22:20:21.123+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:21 INFO TransportClientFactory: Successfully created connection to spark/172.19.0.3:7077 after 31 ms (0 ms spent in bootstraps)
[2025-03-09T22:20:21.296+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:21 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250309222021-0002
[2025-03-09T22:20:21.308+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34529.
[2025-03-09T22:20:21.309+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:21 INFO NettyBlockTransferService: Server created on 27f453beda85:34529
[2025-03-09T22:20:21.310+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-03-09T22:20:21.319+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 27f453beda85, 34529, None)
[2025-03-09T22:20:21.325+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:21 INFO BlockManagerMasterEndpoint: Registering block manager 27f453beda85:34529 with 434.4 MiB RAM, BlockManagerId(driver, 27f453beda85, 34529, None)
[2025-03-09T22:20:21.328+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 27f453beda85, 34529, None)
[2025-03-09T22:20:21.329+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 27f453beda85, 34529, None)
[2025-03-09T22:20:21.616+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:21 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-03-09T22:20:21.892+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:21 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-03-09T22:20:21.894+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:21 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2025-03-09T22:20:24.828+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:24 INFO InMemoryFileIndex: It took 179 ms to list leaf files for 1 paths.
[2025-03-09T22:20:25.287+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:25 INFO InMemoryFileIndex: It took 171 ms to list leaf files for 1 paths.
[2025-03-09T22:20:28.121+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:28 INFO FileSourceStrategy: Pushed Filters:
[2025-03-09T22:20:28.124+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:28 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2025-03-09T22:20:28.853+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:28 INFO CodeGenerator: Code generated in 236.207355 ms
[2025-03-09T22:20:28.939+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2025-03-09T22:20:29.060+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 434.2 MiB)
[2025-03-09T22:20:29.065+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 27f453beda85:34529 (size: 34.9 KiB, free: 434.4 MiB)
[2025-03-09T22:20:29.075+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:29 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
[2025-03-09T22:20:29.103+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4838318 bytes, open cost is considered as scanning 4194304 bytes.
[2025-03-09T22:20:29.332+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:29 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2025-03-09T22:20:29.499+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:29 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-03-09T22:20:29.503+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:29 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2025-03-09T22:20:29.505+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:29 INFO DAGScheduler: Parents of final stage: List()
[2025-03-09T22:20:29.511+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:29 INFO DAGScheduler: Missing parents: List()
[2025-03-09T22:20:29.526+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-03-09T22:20:29.694+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.5 KiB, free 434.2 MiB)
[2025-03-09T22:20:29.705+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.1 MiB)
[2025-03-09T22:20:29.706+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 27f453beda85:34529 (size: 6.4 KiB, free: 434.4 MiB)
[2025-03-09T22:20:29.707+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2025-03-09T22:20:29.734+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-03-09T22:20:29.736+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-03-09T22:20:44.799+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:20:59.765+0000] {spark_submit.py:641} INFO - 25/03/09 22:20:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:21:14.764+0000] {spark_submit.py:641} INFO - 25/03/09 22:21:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:21:29.759+0000] {spark_submit.py:641} INFO - 25/03/09 22:21:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:21:44.765+0000] {spark_submit.py:641} INFO - 25/03/09 22:21:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:21:59.787+0000] {spark_submit.py:641} INFO - 25/03/09 22:21:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:22:14.768+0000] {spark_submit.py:641} INFO - 25/03/09 22:22:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:22:29.762+0000] {spark_submit.py:641} INFO - 25/03/09 22:22:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:22:44.766+0000] {spark_submit.py:641} INFO - 25/03/09 22:22:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:22:59.757+0000] {spark_submit.py:641} INFO - 25/03/09 22:22:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:23:15.134+0000] {spark_submit.py:641} INFO - 25/03/09 22:23:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:23:32.427+0000] {spark_submit.py:641} INFO - 25/03/09 22:23:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:23:46.828+0000] {spark_submit.py:641} INFO - 25/03/09 22:23:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:24:03.425+0000] {spark_submit.py:641} INFO - 25/03/09 22:24:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:24:26.735+0000] {spark_submit.py:641} INFO - 25/03/09 22:24:17 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:24:30.883+0000] {spark_submit.py:641} INFO - 25/03/09 22:24:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:24:33.181+0000] {spark_submit.py:641} INFO - 25/03/09 22:24:33 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@55fa8c14)) by listener AppStatusListener took 11.683147844s.
[2025-03-09T22:24:33.214+0000] {spark_submit.py:641} INFO - 25/03/09 22:24:33 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@55fa8c14)) by listener HeartbeatReceiver took 2.805642426s.
[2025-03-09T22:24:33.550+0000] {job.py:229} INFO - Heartbeat recovered after 83.77 seconds
[2025-03-09T22:24:44.794+0000] {spark_submit.py:641} INFO - 25/03/09 22:24:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:24:59.779+0000] {spark_submit.py:641} INFO - 25/03/09 22:24:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:25:14.764+0000] {spark_submit.py:641} INFO - 25/03/09 22:25:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:25:29.756+0000] {spark_submit.py:641} INFO - 25/03/09 22:25:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:25:44.783+0000] {spark_submit.py:641} INFO - 25/03/09 22:25:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:25:59.778+0000] {spark_submit.py:641} INFO - 25/03/09 22:25:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:26:14.793+0000] {spark_submit.py:641} INFO - 25/03/09 22:26:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:26:29.797+0000] {spark_submit.py:641} INFO - 25/03/09 22:26:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:26:45.064+0000] {spark_submit.py:641} INFO - 25/03/09 22:26:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:27:08.791+0000] {spark_submit.py:641} INFO - 25/03/09 22:27:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:27:17.257+0000] {spark_submit.py:641} INFO - 25/03/09 22:27:17 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:27:17.957+0000] {job.py:229} INFO - Heartbeat recovered after 43.54 seconds
[2025-03-09T22:27:29.947+0000] {spark_submit.py:641} INFO - 25/03/09 22:27:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:27:35.663+0000] {job.py:229} INFO - Heartbeat recovered after 11.56 seconds
[2025-03-09T22:27:45.169+0000] {spark_submit.py:641} INFO - 25/03/09 22:27:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:28:00.680+0000] {spark_submit.py:641} INFO - 25/03/09 22:28:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:28:06.541+0000] {spark_submit.py:641} INFO - 25/03/09 22:28:06 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@24edefba)) by listener AppStatusListener took 1.999582519s.
[2025-03-09T22:28:17.374+0000] {spark_submit.py:641} INFO - 25/03/09 22:28:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:28:19.722+0000] {spark_submit.py:641} INFO - 25/03/09 22:28:19 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@639b78a)) by listener AppStatusListener took 1.578644493s.
[2025-03-09T22:28:30.128+0000] {spark_submit.py:641} INFO - 25/03/09 22:28:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:28:36.780+0000] {spark_submit.py:641} INFO - 25/03/09 22:28:36 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@1c5bb978)) by listener AppStatusListener took 1.049310382s.
[2025-03-09T22:28:45.447+0000] {spark_submit.py:641} INFO - 25/03/09 22:28:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:28:46.325+0000] {job.py:229} INFO - Heartbeat recovered after 70.53 seconds
[2025-03-09T22:29:02.308+0000] {spark_submit.py:641} INFO - 25/03/09 22:29:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:29:10.220+0000] {spark_submit.py:641} INFO - 25/03/09 22:29:02 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@1dc00ccb)) by listener AppStatusListener took 1.403149157s.
[2025-03-09T22:29:14.726+0000] {spark_submit.py:641} INFO - 25/03/09 22:29:14 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@569ba0f6)) by listener AppStatusListener took 1.03077691s.
[2025-03-09T22:29:15.283+0000] {spark_submit.py:641} INFO - 25/03/09 22:29:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:29:14.135+0000] {job.py:239} ERROR - Job heartbeat failed with error
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
           ^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
         ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/job.py", line 233, in heartbeat
    heartbeat_callback(session)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/local_task_job_runner.py", line 284, in heartbeat_callback
    self.task_instance.refresh_from_db()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2359, in refresh_from_db
    _refresh_from_db(task_instance=self, session=session, lock_for_update=lock_for_update)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 872, in _refresh_from_db
    ti = TaskInstance.get_task_instance(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 166, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2345, in get_task_instance
    return query.one_or_none()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2850, in one_or_none
    return self._iter().one_or_none()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
           ^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
         ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-03-09T22:29:16.885+0000] {job.py:254} ERROR - Job heartbeat failed with error. Scheduler is in unhealthy state
[2025-03-09T22:29:23.483+0000] {job.py:229} INFO - Heartbeat recovered after 53.20 seconds
[2025-03-09T22:29:29.999+0000] {spark_submit.py:641} INFO - 25/03/09 22:29:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:29:49.149+0000] {spark_submit.py:641} INFO - 25/03/09 22:29:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:30:08.357+0000] {spark_submit.py:641} INFO - 25/03/09 22:30:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:30:33.277+0000] {spark_submit.py:641} INFO - 25/03/09 22:30:08 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@1dd5d848)) by listener AppStatusListener took 1.925853951s.
[2025-03-09T22:30:41.650+0000] {spark_submit.py:641} INFO - 25/03/09 22:30:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:30:56.524+0000] {spark_submit.py:641} INFO - 25/03/09 22:30:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:31:17.053+0000] {spark_submit.py:641} INFO - 25/03/09 22:30:31 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@5c8432f3)) by listener AppStatusListener took 3.699066132s.
[2025-03-09T22:31:24.138+0000] {spark_submit.py:641} INFO - 25/03/09 22:30:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:31:33.980+0000] {spark_submit.py:641} INFO - 25/03/09 22:30:51 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@413fdbfc)) by listener AppStatusListener took 2.245714476s.
[2025-03-09T22:31:47.026+0000] {spark_submit.py:641} INFO - 25/03/09 22:31:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:31:49.797+0000] {spark_submit.py:641} INFO - 25/03/09 22:31:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:31:52.128+0000] {spark_submit.py:641} INFO - 25/03/09 22:31:17 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@4aa84f3d)) by listener AppStatusListener took 2.857175811s.
[2025-03-09T22:31:54.020+0000] {spark_submit.py:641} INFO - 25/03/09 22:31:31 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@1053f278)) by listener HeartbeatReceiver took 1.000674115s.
[2025-03-09T22:31:56.194+0000] {spark_submit.py:641} INFO - 25/03/09 22:31:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:31:58.441+0000] {spark_submit.py:641} INFO - 25/03/09 22:31:34 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@1053f278)) by listener AppStatusListener took 4.706338555s.
[2025-03-09T22:31:59.511+0000] {spark_submit.py:641} INFO - 25/03/09 22:31:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:32:18.552+0000] {spark_submit.py:641} INFO - 25/03/09 22:31:51 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@790cd9b8)) by listener AppStatusListener took 5.826132604s.
[2025-03-09T22:32:30.521+0000] {spark_submit.py:641} INFO - 25/03/09 22:32:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:32:43.909+0000] {spark_submit.py:641} INFO - 25/03/09 22:32:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:32:56.754+0000] {spark_submit.py:641} INFO - 25/03/09 22:32:27 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@4e26fc0)) by listener AppStatusListener took 8.24164097s.
[2025-03-09T22:33:29.388+0000] {spark_submit.py:641} INFO - 25/03/09 22:32:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:33:34.275+0000] {spark_submit.py:641} INFO - 25/03/09 22:32:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:33:57.201+0000] {spark_submit.py:641} INFO - 25/03/09 22:33:16 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@3b3da49b)) by listener AppStatusListener took 2.880719303s.
[2025-03-09T22:34:17.843+0000] {spark_submit.py:641} INFO - 25/03/09 22:33:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:34:49.575+0000] {spark_submit.py:641} INFO - 25/03/09 22:33:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:34:57.372+0000] {spark_submit.py:641} INFO - 25/03/09 22:33:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:35:05.552+0000] {spark_submit.py:641} INFO - 25/03/09 22:33:49 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:35:10.501+0000] {spark_submit.py:641} INFO - 25/03/09 22:34:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:35:49.693+0000] {spark_submit.py:641} INFO - 25/03/09 22:34:03 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@3364c566)) by listener AppStatusListener took 3.722189478s.
[2025-03-09T22:36:04.961+0000] {spark_submit.py:641} INFO - 25/03/09 22:34:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:36:05.041+0000] {spark_submit.py:641} INFO - 25/03/09 22:34:31 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@5b67b0e7)) by listener HeartbeatReceiver took 1.529530993s.
[2025-03-09T22:36:05.043+0000] {spark_submit.py:641} INFO - 25/03/09 22:34:32 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:36:05.044+0000] {spark_submit.py:641} INFO - 25/03/09 22:34:38 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@5b67b0e7)) by listener AppStatusListener took 5.937606051s.
[2025-03-09T22:36:05.045+0000] {spark_submit.py:641} INFO - 25/03/09 22:34:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:36:05.045+0000] {spark_submit.py:641} INFO - 25/03/09 22:35:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:36:05.046+0000] {spark_submit.py:641} INFO - 25/03/09 22:35:00 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@2921f92c)) by listener HeartbeatReceiver took 2.406395278s.
[2025-03-09T22:36:05.046+0000] {spark_submit.py:641} INFO - 25/03/09 22:35:03 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@2921f92c)) by listener AppStatusListener took 1.727116782s.
[2025-03-09T22:36:05.047+0000] {spark_submit.py:641} INFO - 25/03/09 22:35:17 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:36:05.047+0000] {spark_submit.py:641} INFO - 25/03/09 22:35:27 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@2cba1bab)) by listener AppStatusListener took 3.367564604s.
[2025-03-09T22:36:05.047+0000] {spark_submit.py:641} INFO - 25/03/09 22:35:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:36:05.048+0000] {spark_submit.py:641} INFO - 25/03/09 22:35:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:36:05.048+0000] {spark_submit.py:641} INFO - 25/03/09 22:36:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:36:05.056+0000] {spark_submit.py:641} INFO - 25/03/09 22:36:04 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@3bb1ed93)) by listener AppStatusListener took 3.948031636s.
[2025-03-09T22:36:05.248+0000] {job.py:229} INFO - Heartbeat recovered after 401.85 seconds
[2025-03-09T22:36:14.930+0000] {spark_submit.py:641} INFO - 25/03/09 22:36:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:36:29.756+0000] {spark_submit.py:641} INFO - 25/03/09 22:36:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:36:44.756+0000] {spark_submit.py:641} INFO - 25/03/09 22:36:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:36:59.757+0000] {spark_submit.py:641} INFO - 25/03/09 22:36:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-03-09T22:37:14.757+0000] {spark_submit.py:641} INFO - 25/03/09 22:37:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
